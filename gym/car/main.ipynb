{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess_observation(observation):\n",
    "    \"\"\"\n",
    "    Convert the observation to grayscale and apply edge detection to identify track boundaries.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply Sobel edge detection in both horizontal and vertical directions\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    \n",
    "    # Combine the Sobel X and Y results to get the overall edge magnitude\n",
    "    edges = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # Normalize edges to range 0-1 for easier thresholding\n",
    "    edges_normalized = cv2.normalize(edges, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    \n",
    "    return edges_normalized\n",
    "\n",
    "def simple_policy(edges):\n",
    "    \"\"\"\n",
    "    Determine an action based on the edge detection result. This is a placeholder for a more sophisticated policy.\n",
    "    \"\"\"\n",
    "    # If significant edges are detected in the lower part of the image, it might indicate being off-track.\n",
    "    lower_half_edges = edges[edges.shape[0]//2:, :]\n",
    "    edge_strength = np.mean(lower_half_edges)\n",
    "    # Define a threshold to determine if we are off the track based on edge strength\n",
    "    if edge_strength > 0.06:  # This threshold is arbitrary; adjust based on your observations\n",
    "        return [0, 1, 0]  # Straight with full acceleration\n",
    "    else:\n",
    "        # If we detect less edges, it might indicate being off-track, so try turning\n",
    "        return [np.random.uniform(-1, 1), 0.5, 0]  # Random steering with some acceleration\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=False, render_mode=\"human\")\n",
    "\n",
    "episodes = 3\n",
    "steps = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "\n",
    "    # Main loop\n",
    "    observation, info = env.reset(options={\"randomize\": False})\n",
    "    for _ in range(steps):\n",
    "        # Preprocess the observation to get edge information\n",
    "        edges = preprocess_observation(observation)\n",
    "        \n",
    "        # Decide on an action based on edges\n",
    "        action = simple_policy(edges)\n",
    "        \n",
    "        # Apply the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Blackjack-v1', natural=False, sab=False, render_mode=\"human\")\n",
    "\n",
    "num_states = 10 * 10 * 2  # Adjusted ranges\n",
    "num_actions = 2  # Actions: Stick (0), Hit (1)\n",
    "\n",
    "# Initialize the Q-matrix with zeros\n",
    "Q_matrix = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Define the number of steps you want to simulate\n",
    "episodes = 20\n",
    "steps = 20\n",
    "\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "def update_q_matrix(state_index, action, reward, next_state_index, alpha, gamma):\n",
    "    min_next_q = np.min(Q_matrix[next_state_index])  # Use min for exploration\n",
    "    Q_matrix[state_index, action] = (1 - alpha) * Q_matrix[state_index, action] + alpha * (reward + gamma * min_next_q)\n",
    "\n",
    "\n",
    "def observation_to_state(observation):\n",
    "    player_sum, dealer_card, usable_ace = observation\n",
    "\n",
    "    player_sum -= 12  \n",
    "    dealer_card -= 1 \n",
    "    \n",
    "    # Calculate the state index using base conversion\n",
    "    state_index = player_sum * 10 * 2 + dealer_card * 2 + usable_ace\n",
    "\n",
    "    if(state_index > 200):\n",
    "        return 199\n",
    "    \n",
    "    return state_index\n",
    "\n",
    "win_count = 0\n",
    "loss_count = 0\n",
    "draw_count = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    observation, info = env.reset(options={\"randomize\": False})  # Start a new episode\n",
    "    state_index = observation_to_state(observation)\n",
    "    \n",
    "    for step in range(steps):\n",
    "\n",
    "        action = np.argmin(Q_matrix[state_index])  # Take the action with the smallest Q value (exploration)\n",
    "        next_observation, reward, terminated, _, _ = env.step(action)  # Take the action in the environment\n",
    "        next_state_index = observation_to_state(next_observation)  # Get the state index for the new observation\n",
    "        \n",
    "        # Update the Q-matrix with the new knowledge\n",
    "        update_q_matrix(state_index, action, reward, next_state_index, alpha, gamma)\n",
    "        \n",
    "        env.render()\n",
    "        print(f\"Observation: {next_observation}, Action: {action}, Reward: {reward}, Terminated: {terminated}\")\n",
    "\n",
    "        state_index = next_state_index  # Update the state index for the next step\n",
    "\n",
    "        if terminated:\n",
    "            if reward == 1:\n",
    "                win_count += 1\n",
    "            elif reward == -1:\n",
    "                loss_count += 1\n",
    "            elif reward == 0:\n",
    "                draw_count += 1\n",
    "            break  # Exit the loop because the episode has ended\n",
    "\n",
    "        \n",
    "results_tuple = (win_count, loss_count, draw_count)\n",
    "print(f\"Results - Wins: {win_count}, Losses: {loss_count}, Draws: {draw_count}\")\n",
    "print(f\"Results tuple: {results_tuple}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
